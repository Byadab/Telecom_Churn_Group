{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244143af",
   "metadata": {},
   "source": [
    "# <font color= Purple> 0. Problem statement: Telecom Churn Group study<br>\n",
    "Team-<br> \n",
    "    1. Varun Shenoy<br>\n",
    "    2. Binay Yadab\n",
    "\n",
    "In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n",
    "\n",
    "For many incumbent operators, retaining high profitable customers is the number one business\n",
    "goal. To reduce customer churn, telecom companies need to predict which customers are at high risk of churn. In this project, we should analyze customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn, and identify the main indicators of churn.\n",
    "\n",
    "### Objective   \n",
    "The goal is to build a machine learning model that is able to predict churning customers based on the features provided for their usage.\n",
    "\n",
    "- Identify customers at high risk of churn by building a predicitve ML model\n",
    "- To Identify important churn predictors\n",
    "- Improve the overall accuracy of the model, using different models and explain the business objectives\n",
    "- Recommend different strategies to cointain the churn based on observations from models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae28717",
   "metadata": {},
   "source": [
    "**The Data**\n",
    "\n",
    "Training data in a CSV file along with metadata is provided. The data has 172 columns highlighting the customer behavior, usage, payment, and other patterns that might be relevant. The target variable is \"churn_probability\".\n",
    "\n",
    "Steps followed in solving the Telecom churn case study\n",
    "\n",
    "<b>Step-0 Understanding problem</b> \n",
    "\n",
    "<b>Step-1 Data Understanding (EDA) and visualization:</b> Impute missing value, null value treatment, Univariate and Bivariate analysis, visulaizing the data with appropriate plots\n",
    "\n",
    "<b>Step-2 Data Preperation and modeling:</b> Class imbalance, dummy variables, and scaling, train test split, build different models like, LR, decision Tree classifier, random forest etc\n",
    "\n",
    "<b>Step-3 Model Development and Evaluation:</b> identify the important features and best model.\n",
    "\n",
    "<b>Step-4 Prediction:</b> Predictions on unseen test data  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfda141b",
   "metadata": {},
   "source": [
    "## <font color= Purple> Step-1 Data Understanding (EDA) and visualization:\n",
    "## <font color= Blue> 1.1 Loading dependencies & datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2601fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Structures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "os.chdir('/Volumes/BINAY B/1A-IIITB-UpGrad/EPGP ML C54/Machine learning-2/Unsupervised Learning/telecom-churn-case-study-hackathon-c54')\n",
    "\n",
    "# To hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',10000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# ML libreries of Sklearn and stats model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco = pd.read_csv('train.csv')\n",
    "print(\"Telecom training data :\",len(df_telco))\n",
    "df_telco.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b6db6",
   "metadata": {},
   "source": [
    "## <font color= Blue> 1.2 Understanding the Data (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data description\n",
    "df_telco.describe(percentiles=[0.25, 0.50, 0.75, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51634fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check for row repetation\n",
    "df_telco.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41281900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# capturing rows eith single entries\n",
    "j=0\n",
    "single_entry_list=[]\n",
    "for i in df_telco.columns:\n",
    "    if df_telco[i].nunique() <= 10:\n",
    "        j+=1\n",
    "        print('\\n', df_telco[i].value_counts(),sep=\"\")\n",
    "        if df_telco[i].nunique()==1:\n",
    "            single_entry_list.append(i)\n",
    "print (\"Total columns with less than 11 entries:\",j)\n",
    "print (single_entry_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fdb734",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# to understand meaing of terms\n",
    "df_data_dict = pd.read_csv(\"data_dictionary.csv\")\n",
    "df_data_dict.style #to view all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0698426d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# % Missing value in all columns\n",
    "round(df_telco.isnull().sum()/len(df_telco),4)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e7954",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. training data has 172 columns and 69999 rows\n",
    "2. all data is the numeric formats except date representations which is in MM/DD/YYYY format\n",
    "3. There are outliers present in many columns as seen from describe but they need to be analysed further\n",
    "4. There are no duplicate rows observed in the dataset\n",
    "5. There are 24 columns with less than 11 entries with many having single entry in them.\n",
    "6. Many null values are present in columns A pattern is seen here there are either missing values  under 6% or above 70% in different columns. Further study needed to impute or remove them.\n",
    "7. There is a ID column which is not needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed284c55",
   "metadata": {},
   "source": [
    "## <font color= Blue> 1.3 Missing value treatment (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9065d0",
   "metadata": {},
   "source": [
    "Dropping values with missing values greater than 60% as they cannot be imputed and would leading to skewing of data if imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94593436",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_val_60_list=df_telco.columns[100*(df_telco.isnull().sum()/len(df_telco)) > 60]\n",
    "print(\"Number of columns with more than 60% missing values = \",len(missing_val_60_list))\n",
    "print(missing_val_60_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd9ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items to be retain\n",
    "wanted = {'total_rech_data_6', 'total_rech_data_7', 'total_rech_data_8','av_rech_amt_data_6', 'av_rech_amt_data_7',\n",
    "       'av_rech_amt_data_8'}\n",
    " \n",
    "missing_val_60_list = [ele for ele in missing_val_60_list if ele not in wanted]\n",
    "\n",
    "missing_val_60_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4610de0a",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. Cost columns are retained from the columns with less than 60% missing values\n",
    "2. A new average monthly reccharge amount wil be derived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f53225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco['total_data_rech_6'] = df_telco.total_rech_data_6 * df_telco.av_rech_amt_data_6\n",
    "df_telco['total_data_rech_7'] = df_telco.total_rech_data_7 * df_telco.av_rech_amt_data_7\n",
    "df_telco['total_data_rech_8'] = df_telco.total_rech_data_8 * df_telco.av_rech_amt_data_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dc641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco['amt_data_6'] = df_telco.total_rech_amt_6 + df_telco.total_data_rech_6\n",
    "df_telco['amt_data_7'] = df_telco.total_rech_amt_7 + df_telco.total_data_rech_7\n",
    "df_telco['amt_data_8'] = df_telco.total_rech_amt_8 + df_telco.total_data_rech_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco['av_amt_data_6_7_8'] = (df_telco.amt_data_6 + df_telco.amt_data_7+ df_telco.amt_data_8)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328c0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns from the data set for dropping\n",
    "drop_list=missing_val_60_list+single_entry_list+list(wanted)\n",
    "drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping with id\n",
    "df_telco_v1 = df_telco.drop(drop_list,axis=1)\n",
    "df_telco_v1 = df_telco_v1.drop('id',axis=1)\n",
    "print(\"Telecom dataset before dropping columns:\",df_telco.shape)\n",
    "print(\"Telecom dataset after dropping columns:\",df_telco_v1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab083f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_telco_v1.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752963ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfd97cb",
   "metadata": {},
   "source": [
    "Dates can be dropped as we have monthly reacharge and their amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capturing date columns\n",
    "date_drop=[]\n",
    "for i in df_telco_v1.columns:\n",
    "    if df_telco_v1[i].dtype=='object':\n",
    "        date_drop.append(i)\n",
    "\n",
    "df_telco_v1 = df_telco_v1.drop(date_drop,axis=1)\n",
    "df_telco_v1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39821f16",
   "metadata": {},
   "source": [
    "## <font color= Blue> 1.4 Null value imputaion (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ba63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting columns with null values\n",
    "cols_w_null = df_telco_v1.columns[100*(df_telco_v1.isnull().sum()/len(df_telco_v1)) > 0]\n",
    "print(\"Total Columns with missing values in it = \",len(cols_w_null))\n",
    "print(cols_w_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8eb1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding the distribution for columns with null\n",
    "plt.figure(figsize=(35, 100))\n",
    "for i in range (0,len(cols_w_null)):\n",
    "    plt.subplot(18,5,i+1)\n",
    "    grp= sns.distplot(df_telco_v1[cols_w_null[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae183cb",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. we see that most of the variables have vast distribution but most of their values are zero or close within 100.\n",
    "2. We must not impute them with their means as they are heavily skewed by outliers\n",
    "3. Imputation will be done with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac05a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing with median\n",
    "median_imputation = SimpleImputer(strategy='median', missing_values=np.nan)\n",
    "df_telco_v1[cols_w_null] = median_imputation.fit_transform(df_telco_v1[cols_w_null])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ae7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Missing value in all columns\n",
    "round(df_telco_v1.isnull().sum()/len(df_telco_v1),4)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d9e85",
   "metadata": {},
   "source": [
    "No missing values are observed in any columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c93a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    " df_telco_v1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ac1795",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. We must create a customer profile from this data to identify high value customer, These customers can be a combination of high ARPU (avg revenue per user) and AON (age on network) and average recharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e2c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_dict.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6195d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v1['aon_months'] = df_telco_v1['aon'].apply(lambda x: round((x/365)*12,2))\n",
    "df_telco_v1['aon_months'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d0118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v1['mean_arpu'] = round((df_telco_v1['arpu_6']+df_telco_v1['arpu_7']+df_telco_v1['arpu_8'])/3,2)\n",
    "df_telco_v1['mean_arpu'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a057554",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. Age on network in months tells us the least usage time was ~5 months an maximum of ~142 months on the network \n",
    "2. mean revenue per user for 3 given months is ~280 Units with media ~200 units\n",
    "3. If we combine the aon_months and mean_arpu with average recharge we can get the avg_customer_Spend with the company and all other columns used for this imputation can be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v1['avg_customer_Spend'] = df_telco_v1['aon_months']*df_telco_v1['mean_arpu']*df_telco_v1['av_amt_data_6_7_8']\n",
    "print(df_telco_v1['avg_customer_Spend'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b676a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v1.drop(columns = ['aon','arpu_6','arpu_7','arpu_8','mean_arpu','aon_months','av_amt_data_6_7_8'],axis=1,inplace=True)\n",
    "df_telco_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e5cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "HVC =df_telco_v1['avg_customer_Spend'].quantile(0.7)#High value customers\n",
    "LVC = df_telco_v1['avg_customer_Spend'].quantile(0.4) #low value customers\n",
    "df_telco_v1['customer_value'] = df_telco_v1['avg_customer_Spend'].apply(\n",
    "    lambda x: 'HVC' if x > HVC else ('LVC' if x < LVC else 'MVC')) # MVC- Medium value customers\n",
    "df_telco_v1['customer_value'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af63e2d9",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. We now have a customer vaulue and who the telecom operator has to focus on for maximum retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cf037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v2=df_telco_v1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d724d76",
   "metadata": {},
   "source": [
    "## <font color= Blue> 1.5 Outlier treatment and Visualization (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869179b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "for col in df_telco_v2.columns: \n",
    "    if is_numeric_dtype(df_telco_v2[col]):\n",
    "        plt.title(col)\n",
    "        sns.boxplot(x = df_telco_v2['customer_value'],y=df_telco_v2[col], data = df_telco_v2)\n",
    "        print (\"maximum value for\",col, \"is:\",df_telco_v2[col].max(),\n",
    "               \"\\nminimum value for\",col, \"is:\", df_telco_v2[col].min(),\n",
    "              \"\\nmedian value for\",col, \"is:\", df_telco_v2[col].quantile(0.50))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1590dbd9",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. Almost all columns have outliers. \n",
    "2. Outliers will not be removed from all columns e.x columns like SACHET, monthly_3g, spl etc have very low difference between max and median \n",
    "3. Many columns like std recharge, roaming cannot be considered in outlier analysis are they are a cause for revenue and dissatisfaction here may lead to more churn\n",
    "3. Hence outliers are dropped only from these columns and 95 percentile is used as outlier threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_cols=['total_ic_mou_6','total_og_mou_6','onnet_mou_6','offnet_mou_6','total_rech_amt_6','total_ic_mou_7','total_og_mou_7','onnet_mou_7','offnet_mou_7','total_rech_amt_7','total_ic_mou_8','total_og_mou_8','onnet_mou_8','offnet_mou_8','total_rech_amt_8',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca17ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v1[outlier_cols].describe(percentiles=[0,0.25,0.5,0.75,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca015be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in outlier_cols:\n",
    "    if is_numeric_dtype(df_telco_v2[col]):\n",
    "        df_telco_v2=df_telco_v2[df_telco_v2[col]<df_telco_v2[col].quantile(0.95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3ecd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919cb91b",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. Among the identified outliers only the very high deviation cases i.e ratio between max and 99percentile is more than 6times only those entries are removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43aef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(((69999-df_telco_v2.shape[0])/69999)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7112ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd720fe",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. few key columns were identified and outliers seen in those columns were eliminated resulting trimming dataset by ~53%.\n",
    "2. This is a risk which will taken at this point as we will focus on accuracy of the model to predict churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59796165",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "df_telco_v2['churn_probability'].value_counts().plot.bar()\n",
    "values = [df_telco_v2['churn_probability'].value_counts()[0],df_telco_v2['churn_probability'].value_counts()[1]]\n",
    "plt.plot(range(len(values)), values, 'r+')\n",
    "plt.title(\"Churn Probability\")\n",
    "for i, v in enumerate(values):\n",
    "    ax.text(i, v, \"%d\" %v, ha=\"center\")\n",
    "plt.ylim(0,35000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ee245",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 3))\n",
    "sns.scatterplot(x = 'avg_customer_Spend', y ='churn_probability', data = df_telco_v2, hue='customer_value')\n",
    "plt.xlim(left=0,right=25000000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9f8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# of churn in the filtered dataset\n",
    "round((df_telco_v2['churn_probability'].value_counts()[1]/df_telco_v2['churn_probability'].value_counts()[0])*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53388e00",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "1. A big class imablance is seen between churn (14.25%) and active cases (85.75%). \n",
    "2. correlation matirx is carried out to understand if there are any visible  patterns correlating with churn and spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467db777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation plot of whole dataset\n",
    "plt.figure(figsize = (40, 40))\n",
    "sns.heatmap(df_telco_v2.corr(), cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b58f8",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. since the grph is not cler the stdy is carried out in Month-wise order to see if there are any visible patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e33326",
   "metadata": {},
   "outputs": [],
   "source": [
    "June_month =[]\n",
    "July_month = []\n",
    "August_month = []\n",
    "others=[]\n",
    "for i in df_telco_v2.columns:\n",
    "    if '6' in i or 'jun' in i.lower():\n",
    "        June_month.append(i)\n",
    "    elif '7' in i or 'jul' in i.lower():\n",
    "        July_month.append(i)\n",
    "    elif '8' in i or 'aug' in i.lower():\n",
    "        August_month.append(i)\n",
    "    else:\n",
    "        others.append(i)\n",
    "\n",
    "print(June_month, len(June_month))\n",
    "print(July_month, len(July_month))\n",
    "print(August_month, len(August_month))\n",
    "print(others)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c3a1b",
   "metadata": {},
   "source": [
    "Analysis Month wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafdb5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(June_month)):\n",
    "    plt.figure(figsize=(80, 60))\n",
    "    plt.subplot(11,4,i+1)\n",
    "    sns.scatterplot(x = df_telco_v2[June_month[i]], y = 'avg_customer_Spend', data = df_telco_v2, hue='customer_value',style='churn_probability',size='churn_probability',sizes=(200,20))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628b094a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(July_month)):\n",
    "    plt.figure(figsize=(80, 60))\n",
    "    plt.subplot(11,4,i+1)\n",
    "    sns.scatterplot(x = df_telco_v2[July_month[i]], y = 'avg_customer_Spend', data = df_telco_v2, hue='customer_value',style='churn_probability',size='churn_probability',sizes=(200,20))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bded24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(August_month)):\n",
    "    plt.figure(figsize=(80, 60))\n",
    "    plt.subplot(11,4,i+1)\n",
    "    sns.scatterplot(x = df_telco_v2[August_month[i]], y = 'avg_customer_Spend', data = df_telco_v2, hue='customer_value',style='churn_probability',size='churn_probability',sizes=(200,20))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c063f",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. Visually more churns ca be seen in high value customers in std_outgoing calls followed by roaming and recharge amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162bd3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(month):\n",
    "    plt.figure(figsize = (40, 40))\n",
    "    sns.heatmap(df_telco_v2[month].corr(), annot=True, cmap=\"YlGnBu\")\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587fc7c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "June=June_month+others\n",
    "corr(June)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283f7c39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "July=July_month+others\n",
    "corr(July)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd33e2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Aug=August_month+others\n",
    "corr(Aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea94643",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. We can see small patches where customer spend is positive and aslo the churn posibility is positive although very slightly these variable are important\n",
    "2. Also when the spending is high the  churn probaility is negetive\n",
    "3. from the scatter plot it is also evident that the sd_og, total og, roaming is an importaint variable\n",
    "4. Most churns occur in August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7daced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.countplot(x=\"customer_value\",hue=\"churn_probability\", data=df_telco_v2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7f2adc",
   "metadata": {},
   "source": [
    "##  <font color= Green> Key insights from EDA -\n",
    " - Most customers who are highvalue as per our analysis have a lower churn rate.\n",
    " - Targeting the middle MVC and HVC is benefetial to the company\n",
    " - Mostly the churn probabilty is negetively correlated with most variables which means predictor variables donot show a clear pattern with the target variable\n",
    " - There is a big class imbalance between the most of 88% of the data is active users\n",
    " - only few variables(like std_og, roam etc) gives a clear identification how it is related to revenue and churn\n",
    " - In this dataset august sees highest churn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d06816",
   "metadata": {},
   "source": [
    "## <font color= Purple> Step-2 Data Preperation and modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a9c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v2.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11981619",
   "metadata": {},
   "source": [
    "Handling class imbalance by resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e43bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_telco_v2['customer_value'] = df_telco_v2['customer_value'].apply(lambda x:2 if x=='HVC' else (0 if x=='LVC' else 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40370891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Test Train split\n",
    "# y = df_telco_v2.loc[:,'churn_probability']\n",
    "# x = df_telco_v2.drop(columns=['churn_probability'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b71329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smo = SMOTE(random_state=15)\n",
    "# X_rsamp, y_rsamp = smo.fit_resample(x, y)\n",
    "# print(X_rsamp.shape, y_rsamp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0665d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_telco_v3 = df_telco_v2.copy()\n",
    "# df_telco_v3 = pd.concat([X_rsamp,y_rsamp],axis=1)\n",
    "# df_telco_v3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c77074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_telco_v3['customer_value'] = df_telco_v3['customer_value'].apply(lambda x:'HVC' if x==2 else ('LCV' if x==0 else 'MCV'))\n",
    "df_telco_v2['customer_value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e6e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9026d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.countplot(x=\"customer_value\",hue=\"churn_probability\", data=df_telco_v2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a518db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of churn in the resampled dataset\n",
    "round((df_telco_v2['churn_probability'].value_counts()[1]/df_telco_v2['churn_probability'].value_counts()[0])*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefdec50",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. Usually resamplig is not preffered as the dataset observes lower churn % compared to active users which means it depicts reality and is actually benefitial to the company.\n",
    "2. Since the goal is to improve accuracy of prediction along with recall and precesion. Resampling is performed \n",
    "3. After resampling we saw the churn probailities are equally distributed and imbalance is eliminated\n",
    "    \n",
    "P.S Resampling was removed after seeing seeing similar kaggle scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed9e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode\n",
    "hot_encode = pd.get_dummies(df_telco_v2['customer_value'], drop_first=True, prefix='customer_value')\n",
    "df_telco_v2.drop(columns=['customer_value'],axis=1,inplace=True)\n",
    "df_telco_v2 = pd.concat([df_telco_v2,hot_encode],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8773e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba77c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_telco_v2,train_size=0.80,test_size=0.20,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d520bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.pop('churn_probability')\n",
    "X_train = df_train\n",
    "y_test = df_test.pop('churn_probability')\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5970ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_telco_v2.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e2e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation is used to Scale and split the train & test datasets\n",
    "scaler = StandardScaler()\n",
    "var=X_train.columns\n",
    "X_train[var] = scaler.fit_transform(X_train[var])\n",
    "X_test[var] = scaler.transform(X_test[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20164683",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95915de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa1a2a8",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. All values seems to be scaled in the same similarly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047bb41",
   "metadata": {},
   "source": [
    "## <font color= Purple> Step-3 Model Development and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdaab8d",
   "metadata": {},
   "source": [
    "### <font color= Blue> 3.1 Logistic regression is chosen as base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9171bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic = LogisticRegression()\n",
    "model_logistic.fit(X_train, y_train)\n",
    "y_pred_log =  model_logistic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cc79f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(y_test,y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print('The confusion Matrix : \\n',cm)\n",
    "    \n",
    "    TP = cm[1,1] # true positives \n",
    "    TN = cm[0,0] # true negatives\n",
    "    FP = cm[0,1] # false positives\n",
    "    FN = cm[1,0] # false negatives\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_true=y_test,y_pred=y_pred)\n",
    "    recall = TP/(FN+TP)\n",
    "    specificity = TN/(TN+FP)\n",
    "    precision = TP/(FP+TP)\n",
    "\n",
    "    print(\"Accuracy = {:.2f}\".format(accuracy))\n",
    "    print(\"Sensitivity/Recall = {:.2f}\".format(recall))\n",
    "    print(\"Specificity = {:.2f}\".format(specificity))\n",
    "    print(\"Precision = {:.2f}\".format(precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test,y_pred_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c92b0",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. The base model itself is got with very high accuracy of 93%\n",
    "2. all other scores seems to be lesser than accuracy score with recall being at 59%\n",
    "3. Just to get a fair idea VIF is applied and some columns are dropped just to see the influence on the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "## eliminating features by performing VIF\n",
    "\n",
    "def VIF_calc(dataframe):\n",
    "    vif = pd.DataFrame()\n",
    "    vif['Features'] = dataframe.columns\n",
    "    vif['VIF'] = [variance_inflation_factor(dataframe.values, i) for i in range(dataframe.shape[1])]\n",
    "    vif['VIF'] = round(vif['VIF'], 2)\n",
    "    vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "    return vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bbf502",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_df=VIF_calc(X_train[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72a450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_df_rem_Feature=vif_df[vif_df.VIF>1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf65d05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "removal_list=vif_df_rem_Feature.Features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38809c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(removal_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ea9fd",
   "metadata": {},
   "source": [
    "# <font color= Orange> Observations\n",
    "\n",
    "1. VIF shows more than 60 columns with a very high VIF value\n",
    "2. This would take many iterations to solve\n",
    "3. these 60 colums with high VIF are dropped just to understand its influence and see the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_1=var.drop(removal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2e228",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic.fit(X_train[var_1], y_train)\n",
    "y_pred_log_2 =  model_logistic.predict(X_test[var_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa55f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test,y_pred_log_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bde1dc",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. Accurachy has reduced to 91%. this means we must be careful in removing the columns just by VIF scrores although high multicollinearity is present.\n",
    "2. The feature elimination will be retried after PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a088a",
   "metadata": {},
   "source": [
    "### <font color= Blue> 3.2 Decision Tree base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree = DecisionTreeClassifier(max_depth=3,random_state=100)\n",
    "dec_tree.fit(X_train[var], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image  \n",
    "from six import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus, graphviz\n",
    "\n",
    "def get_graph(classifier):\n",
    "    dot_data = StringIO()  \n",
    "    export_graphviz(classifier, out_file=dot_data, filled=True, rounded=True,\n",
    "                    feature_names=X_train.columns, \n",
    "                    class_names=['Churn', \"Active\"])\n",
    "\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "    return Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_graph(dec_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c351f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dec_tree_base = dec_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055df4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test,y_pred_dec_tree_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9ca1a",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. decision tree gives gives the best model in accuracy so far compared to linear regression base model\n",
    "2. the recall has increased to 69% and improvement in precision and specificity are seen\n",
    "3. Dimentionality is further reduced by using PCA in the next steps, logistic regression and decision tree classifier is perormed again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20192e8",
   "metadata": {},
   "source": [
    "### <font color= Blue> 3.3 Principal Component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b4308",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70975dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed0dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada8baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative sum calculation for scree plot to understand variance v/s variables\n",
    "cumsum_vars = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb4dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,6])\n",
    "plt.vlines(x=75, ymax=100, ymin=0, colors=\"g\", linestyles=\"--\")\n",
    "plt.hlines(y=95, xmax=125, xmin=0, colors=\"r\", linestyles=\"-.\")\n",
    "plt.plot(cumsum_vars)\n",
    "plt.ylabel(\"Cumulative variance\")\n",
    "plt.xlabel(\"Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b28b3ee",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. from the scree plot we can see that 75 variables can represent 95% variation in the data i.e roughly just above half the dataset size.\n",
    "2. This wil be taken forward in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_75_var = PCA(n_components=75,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d209773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_75 = pca_75_var.fit_transform(X_train)\n",
    "X_test_pca_75 = pca_75_var.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_pca_75.shape)\n",
    "print(X_test_pca_75.shape)\n",
    "X_train_pca_75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9139ec9e",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. data set has been reduced to 75 variable array after pca\n",
    "2. Logistic regression,Decision tree and random forest are done on this data to calculate the matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e3203f",
   "metadata": {},
   "source": [
    "### <font color= Blue> 3.4 Logistic Regression after PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiating logistic regression stats model is used to understand summary\n",
    "X_train_pca_75_sm = sm.add_constant(X_train_pca_75)\n",
    "logrg = sm.GLM(y_train,X_train_pca_75_sm, family=sm.families.Binomial()).fit()\n",
    "logrg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_75_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e5da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_LR_PCA_pred = logrg.predict(X_train_pca_75_sm)\n",
    "y_train_LR_PCA_pred_df = pd.DataFrame(y_train_LR_PCA_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4257be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_LR_PCA_pred_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ca7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_pred\n",
    "y_train_LR_PCA_pred_df.columns = ['pred_proba']\n",
    "y_train_LR_PCA_pred_df['pred_class'] = y_train_LR_PCA_pred_df['pred_proba'].map(lambda x: 1 if x > 0.5 else 0)\n",
    "get_confusion_matrix(y_train,y_train_LR_PCA_pred_df['pred_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9b857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca_75_sm = sm.add_constant(X_test_pca_75)\n",
    "y_test_LR_PCA_pred = logrg.predict(X_test_pca_75_sm)\n",
    "y_test_LR_PCA_pred_df = pd.DataFrame(y_test_LR_PCA_pred)\n",
    "y_test_LR_PCA_pred_df.columns = ['pred_proba']\n",
    "y_test_LR_PCA_pred_df['pred_class'] = y_test_LR_PCA_pred_df['pred_proba'].map(lambda x: 1 if x > 0.5 else 0)\n",
    "get_confusion_matrix(y_test,y_test_LR_PCA_pred_df['pred_class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d2a053",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. The PCA fit Logistic regression model gives a test accuracy of 92% which is better than base. Recall and precision has reduced "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58047877",
   "metadata": {},
   "source": [
    "### <font color= Blue> 3.4 Decision tree calssifier with PCA and Hyper tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6eef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree_cla = DecisionTreeClassifier(random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06244f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "params_dec_tree = {\n",
    "    'max_depth': [ 3, 5, 10, 20],\n",
    "    'min_samples_leaf': [5, 10, 20, 50, 100, 1000],\n",
    "    'criterion': [\"gini\", \"entropy\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3697e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=dec_tree_cla, \n",
    "                           param_grid=params_dec_tree, \n",
    "                           cv=4, n_jobs=-1, verbose=1, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa8a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train_pca_75, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ec4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dec_tree = pd.DataFrame(grid_search.cv_results_)\n",
    "score_dec_tree.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61af4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dec_tree.nlargest(5,\"mean_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1f0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06379046",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree_final = DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_leaf=100, random_state=100)\n",
    "dec_tree_final.fit(X_train_pca_75, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a387520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dec_tree = dec_tree_final.predict(X_test_pca_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test,y_pred_dec_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb99121",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. The accracy has marginally reduced from 89% from base decisoin tree model  \n",
    "2. but the other matries have also marginally decreased on all counts except recall.\n",
    "3. Hence is this case the base model decision tree performs better on all counts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa948afe",
   "metadata": {},
   "source": [
    "### <font color= Blue> 3.5 Random forest Calssifer with PCA and hyper tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342499a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rf = {\n",
    "    'max_depth': [ 3, 5, 10, 20],\n",
    "    'min_samples_leaf': [5, 10, 20, 50, 100],\n",
    "    'n_estimators': [10, 25, 50, 100,200]\n",
    "}\n",
    "# Instantiate the grid search model\n",
    "rf = RandomForestClassifier(random_state=100)\n",
    "grid_search = GridSearchCV(estimator=rf, \n",
    "                           param_grid=params_rf, \n",
    "                           cv=4, n_jobs=-1, verbose=1, scoring = \"recall\")\n",
    "\n",
    "grid_search.fit(X_train_pca_75, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b921146",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pca_final = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87066f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pca_final.fit(X_train_pca_75,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cbf32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rf_pca_final.predict(X_test_pca_75)\n",
    "get_confusion_matrix(y_test,y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c46bc4",
   "metadata": {},
   "source": [
    "## <font color= Orange> Observations\n",
    "\n",
    "1. Random forest classifier gives better results compared to decision tree on all counts\n",
    "2. The best model observed so far is decision tree base model in accuracy and presesion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3267609",
   "metadata": {},
   "source": [
    "## <font color= Green> Key Insights Model Development and Evaluation  -\n",
    "1. The base linear regression model was itself very good interms of prediction with 92% accuracy and good scores in precision and specificity \n",
    "2. If Recall needs to be improved resampling can be done this has been tried and except accuracy all other matrices improve\n",
    "4. Also differnt models like Logistic regreesion with VIF, Decision tree (base and tuned), Random Forest (tuned) were performed\n",
    "5. Best results were observed with PCA and hypertuned Random forest classifer with following matrices\n",
    "    Accuracy = 0.92\n",
    "    Sensitivity/Recall = 0.35\n",
    "    Specificity = 0.99\n",
    "    Precision = 0.80\n",
    "    \n",
    "altough very best model was decision tree base model (this cannot be used as it may be prone to overfitting)\n",
    "    Accuracy = 0.94\n",
    "    Sensitivity/Recall = 0.69\n",
    "    Specificity = 0.97\n",
    "    Precision = 0.80\n",
    "    \n",
    "this means The classifier detects all the churn cases as churn 35% of the times (Recall) and active cases as active 99% of the time (specificity). Precision also indicates that the cases which were dected churn are actually churn with 80% precision.\n",
    "6. Random forest with hyper tuning of depth 20, sample leaf 5, n estimator 200 is used for final submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08299ece",
   "metadata": {},
   "source": [
    "## <font color= Purple> FINAL PREDICTION (UNSEEN DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c68f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read test csv\n",
    "df_telco_test = pd.read_csv(\"test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture customer id\n",
    "Final_pred=pd.DataFrame()\n",
    "Final_pred['id']=df_telco_test.id\n",
    "Final_pred.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2184e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1=df_telco_test.copy()\n",
    "df_telco_test_v1.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b2302",
   "metadata": {},
   "source": [
    "Applying all the Pre-processing on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1['total_data_rech_6'] = df_telco_test_v1.total_rech_data_6 * df_telco_test_v1.av_rech_amt_data_6\n",
    "df_telco_test_v1['total_data_rech_7'] = df_telco_test_v1.total_rech_data_7 * df_telco_test_v1.av_rech_amt_data_7\n",
    "df_telco_test_v1['total_data_rech_8'] = df_telco_test_v1.total_rech_data_8 * df_telco_test_v1.av_rech_amt_data_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce149da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1['amt_data_6'] = df_telco_test_v1.total_rech_amt_6 + df_telco_test_v1.total_data_rech_6\n",
    "df_telco_test_v1['amt_data_7'] = df_telco_test_v1.total_rech_amt_7 + df_telco_test_v1.total_data_rech_7\n",
    "df_telco_test_v1['amt_data_8'] = df_telco_test_v1.total_rech_amt_8 + df_telco_test_v1.total_data_rech_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15916065",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1['av_amt_data_6_7_8'] = (df_telco_test_v1.amt_data_6 + df_telco_test_v1.amt_data_7+ df_telco_test_v1.amt_data_8)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ef06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1 = df_telco_test_v1.drop(drop_list,axis=1)\n",
    "df_telco_test_v1 = df_telco_test_v1.drop('id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58939412",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1 = df_telco_test_v1.drop(date_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c80f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing with median\n",
    "cols_w_null = df_telco_test_v1.columns[100*(df_telco_test_v1.isnull().sum()/len(df_telco_test_v1)) > 0]\n",
    "df_telco_test_v1[cols_w_null] = median_imputation.fit_transform(df_telco_test_v1[cols_w_null])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6aa487",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b8de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1['aon_months'] = df_telco_test_v1['aon'].apply(lambda x: round((x/365)*12,2))\n",
    "df_telco_test_v1['mean_arpu'] = round((df_telco_test_v1['arpu_6']+df_telco_test_v1['arpu_7']+df_telco_test_v1['arpu_8'])/3,2)\n",
    "df_telco_test_v1['avg_customer_Spend'] = df_telco_test_v1['aon_months']*df_telco_test_v1['mean_arpu']*df_telco_test_v1['av_amt_data_6_7_8']\n",
    "df_telco_test_v1.drop(columns = ['aon','arpu_6','arpu_7','arpu_8','mean_arpu','aon_months','av_amt_data_6_7_8'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7997b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1['customer_value'] = df_telco_test_v1['avg_customer_Spend'].apply(\n",
    "    lambda x: 'HVC' if x > HVC else ('LVC' if x < LVC else 'MVC')) # MVC- Medium value customers\n",
    "df_telco_test_v1['customer_value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2588409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_telco_test_v1['customer_value'] = df_telco_test_v1['customer_value'].apply(lambda x:2 if x=='HVC' else (0 if x=='LVC' else 1))\n",
    "\n",
    "hot_encode = pd.get_dummies(df_telco_test_v1['customer_value'], drop_first=True, prefix='customer_value')\n",
    "df_telco_test_v1.drop(columns=['customer_value'],axis=1,inplace=True)\n",
    "df_telco_test_v1 = pd.concat([df_telco_test_v1,hot_encode],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ac7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36edd7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_telco_test_v2=df_telco_test_v1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fd2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "var=df_telco_test_v1.columns\n",
    "df_telco_test_v1[var] = scaler.transform(df_telco_test_v1[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1_pca_75 = pca_75_var.transform(df_telco_test_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef384bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pred['churn_probability'] = dec_tree_final.predict(df_telco_test_v1_pca_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pred.to_csv(\"Submission_Pred_test_Dec_Tree_13_dec_CV5.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecaf2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pred=Final_pred.drop('churn_probability',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a640a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e87d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pred['churn_probability'] = rf_pca_final.predict(df_telco_test_v1_pca_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda31c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pred.to_csv(\"Submission_Pred_test_Random_forest_13_dec_CV5.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fea26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pred=Final_pred.drop('churn_probability',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152157f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_telco_test_v1_pca_75_sm = sm.add_constant(df_telco_test_v1_pca_75)\n",
    "y_test_LR_PCA_pred_unseen = logrg.predict(df_telco_test_v1_pca_75_sm)\n",
    "y_test_LR_PCA_pred_unseen_df = pd.DataFrame(y_test_LR_PCA_pred_unseen)\n",
    "y_test_LR_PCA_pred_unseen_df.columns = ['pred_proba']\n",
    "y_test_LR_PCA_pred_unseen_df['churn_probability'] = y_test_LR_PCA_pred_unseen_df['pred_proba'].map(lambda x: 1 if x > 0.5 else 0)\n",
    "Final_pred=pd.concat([Final_pred['id'],y_test_LR_PCA_pred_unseen_df['churn_probability']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c9a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pred.to_csv(\"Submission_Pred_test_LR_13_dec_CV5.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434a151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pred['churn_probability'] = dec_tree.predict(df_telco_test_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pred.to_csv(\"Submission_Pred_test_base_dec_tree_13_dec_CV5.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae59ee",
   "metadata": {},
   "source": [
    "## <font color= Green> Suggestions to the company\n",
    " - Most customers who are highvalue as per our analysis have a lower churn rate.\n",
    " - Although highvalue + Middle value customers together have a churn rate of above 60% and Targeting them is benefetial to the company\n",
    " - better offers around roaming and outgoing standard calls will help reducing the churn in this segment\n",
    " - Among the given dataset company can target month of June for releasing new offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d534f732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
